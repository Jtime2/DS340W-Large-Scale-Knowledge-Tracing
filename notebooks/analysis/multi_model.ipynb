{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('vedu': conda)"
  },
  "interpreter": {
   "hash": "5de9c6bbfc24e23fab00c3f9f8dc98e9bcec0a8369077ced4c7728b8be480d9b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation of multiple models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "sns.set()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def compute_metrics(y_pred, y, protection=1e-8):\n",
    "    \"\"\"\n",
    "    Compute accuracy, AUC score, Negative Log Loss, MSE and F1 score\n",
    "    \"\"\"\n",
    "    # print(y_pred.min(), y_pred.max(), y_pred.shape)\n",
    "    y_pred = np.array([i if np.isfinite(i) else 0.5 for i in y_pred])\n",
    "    acc = accuracy_score(y, y_pred >= 0.5)\n",
    "    auc = roc_auc_score(y, y_pred)\n",
    "    return acc, auc\n",
    "\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Keep track of metrics over time in a dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "        self.counts = {}\n",
    "\n",
    "    def store(self, new_metrics):\n",
    "        for key in new_metrics:\n",
    "            if key in self.metrics:\n",
    "                self.metrics[key] += new_metrics[key]\n",
    "                self.counts[key] += 1\n",
    "            else:\n",
    "                self.metrics[key] = new_metrics[key]\n",
    "                self.counts[key] = 1\n",
    "\n",
    "    def average(self):\n",
    "        average = {k: v / self.counts[k] for k, v in self.metrics.items()}\n",
    "        self.metrics, self.counts = {}, {}\n",
    "        return average\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "DATASETS = [\"squirrel\", \"ednet_kt3\", \"eedi\", \"junyi_15\"]\n",
    "SEED = 888\n",
    "dataset = \"squirrel\"\n",
    "\n",
    "DATASET_NAMES = {\n",
    "    \"squirrel\": \"Squirrel\",\n",
    "    \"ednet_kt3\": \"EdNet KT3\",\n",
    "    \"eedi\": \"Eedi\",\n",
    "    \"junyi_15\": \"Junyi15\"\n",
    "}\n",
    "\n",
    "splits = []\n",
    "for i in range(5):\n",
    "    path = \"./../../data/\" + dataset + \"/preparation/split_s\" + str(SEED) + \"_\" + str(i) + \".pkl\"\n",
    "    with open(path, \"rb\") as file_object:\n",
    "        s = pickle.load(file_object)\n",
    "    splits.append(s)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Determine ACC and AUC of multi model evaluation\n",
    "\n",
    "def get_eval(p, dataset, suf, splits, split_id):\n",
    "    path = \"partitioning/\" + dataset + \"_\" + p + \"_s\" + str(split_id) + \"_\" + suf + \".pkl\"\n",
    "    train_selector = splits[split_id][\"selector_train\"]\n",
    "    test_selector = splits[split_id][\"selector_test\"]\n",
    "    \n",
    "    with open(path, 'rb') as f:\n",
    "        res_dict = pickle.load(f)\n",
    "\n",
    "    y_pred_tr = res_dict[\"y_pred_train\"][train_selector]\n",
    "    y_truth_tr = res_dict[\"y_truth_train\"][train_selector]\n",
    "    y_pred_te = res_dict[\"y_pred_test\"][test_selector]\n",
    "    y_truth_te = res_dict[\"y_truth_teest\"][test_selector]\n",
    "\n",
    "    acc_train, auc_train = \\\n",
    "            compute_metrics(y_pred_tr, y_truth_tr)\n",
    "\n",
    "    acc_test, auc_test = \\\n",
    "        compute_metrics(y_pred_te, y_truth_te)\n",
    "\n",
    "    return acc_train, auc_train, acc_test, auc_test\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def comb_lr_performance(ps, dataset, suf, splits, split_id):\n",
    "    \n",
    "    # combine predictions\n",
    "    train_selector = splits[split_id][\"selector_train\"]\n",
    "    test_selector = splits[split_id][\"selector_test\"]\n",
    "\n",
    "    pred_tr, pred_te = [], []\n",
    "    for p in ps:\n",
    "        print(p)\n",
    "        path = \"partitioning/\" + dataset + \"_\" + p + \"_s\" + str(split_id) + \"_\" + suf + \".pkl\"\n",
    "        with open(path, 'rb') as f:\n",
    "            res_dict = pickle.load(f)\n",
    "\n",
    "        y_pred_tr = res_dict[\"y_pred_train\"][train_selector]\n",
    "        y_truth_tr = res_dict[\"y_truth_train\"][train_selector]\n",
    "        y_pred_te = res_dict[\"y_pred_test\"][test_selector]\n",
    "        y_truth_te = res_dict[\"y_truth_teest\"][test_selector]\n",
    "        \n",
    "        pred_tr.append(y_pred_tr) \n",
    "        pred_te.append(y_pred_te)\n",
    "\n",
    "    X_train = np.array(pred_tr).T\n",
    "    y_train = y_truth_tr\n",
    "    X_test = np.array(pred_te).T\n",
    "    y_test = y_truth_te\n",
    "\n",
    "    lr_model = LogisticRegression(solver=\"liblinear\",\n",
    "                            max_iter=5000,\n",
    "                            n_jobs=8,\n",
    "                            verbose=1)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    pred_tr = lr_model.predict_proba(X_train)[:, 1]\n",
    "    pred_te = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc_train, auc_train = \\\n",
    "        compute_metrics(pred_tr, y_train)\n",
    "    acc_test, auc_test = \\\n",
    "        compute_metrics(pred_te, y_test)\n",
    "\n",
    "    return acc_train, auc_train, acc_test, auc_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Individual split performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Best-LR\n",
    "suf = \"i_s_scA_scW_tcA_tcW\"\n",
    "# squirrel\n",
    "partitions = [\"single\", \"time\", \"i\", \"s\", \"sm\", \"tea\", \"sch\", \"c\", \"t\", \"at\"]\n",
    "# ednet\n",
    "# partitions = [\"single\", \"time\", \"i\", \"hashed_skill_id\", \"sm\", \"bundle_id\", \"part_id\", \"at\"]\n",
    "# eedi\n",
    "# [\"single\", \"time\", \"i\", \"hashed_skill_id\", \"sm\", \"tea\", \"bundle_id\"]\n",
    "# junyi\n",
    "# #partitions = [\"single\", \"time\", \"i\", \"s\", \"sm\", \"part_id\"]\n",
    "\n",
    "\n",
    "### Informed LR\n",
    "# squirrel\n",
    "suf = \"i_icA_TW_icW_TW_lag_time_cat_n_gram_postcA_postcW_precA_precW_prev_\" \\\n",
    "    + \"resp_time_cat_rc_s_scA_TW_scW_TW_sm_t_tcA_TW_tcW_TW_user_avg_correct_vw\"\n",
    "# ednet\n",
    "# suf = \"i_icA_TW_icW_TW_lag_time_cat_n_gram_partcA_partcW_\" \\\n",
    "#     + \"prev_resp_time_cat_s_scA_TW_scW_TW_sm_tcA_TW_tcW_TW_user_avg_correct_vw\"\n",
    "# partitions = [\"single\", \"time\", \"i\", \"hashed_skill_id\", \"sm\", \"bundle_id\", \"part_id\", \"at\"]\n",
    "# eedi\n",
    "# suf = \"bundle_i_icA_TW_icW_TW_n_gram_precA_precW_s_scA_TW_scW_TW_sm\" \\\n",
    "#     + \"_tcA_TW_tcW_TW_tea_user_avg_correct\"\n",
    "# junyi\n",
    "#suf = \"hour_i_icA_TW_icW_TW_lag_time_cat_n_gram_postcA_postcW_precA_\" \\\n",
    "#    + \"precW_prev_resp_time_cat_rc_s_scA_TW_scW_TW_sm_tcA_TW_tcW_TW_\" \\\n",
    "#    + \"user_avg_correct\"\n",
    "# partitions = [\"c\", \"t\", \"at\"]\n",
    "\n",
    "print(dataset)\n",
    "for p in partitions:\n",
    "    print(\"Partition: \" + p)\n",
    "    print(\"\")\n",
    "    acc_vals, auc_vals = [], []\n",
    "    for split_id in range(5):  #  range(5)  [0, 1, 3, 4]\n",
    "        print(\"Split\", split_id)\n",
    "        acc_train, auc_train, acc_test, auc_test = \\\n",
    "            get_eval(p, dataset, suf, splits, split_id)\n",
    "        acc_vals.append(acc_test)\n",
    "        auc_vals.append(auc_test)\n",
    "\n",
    "    acc_vals = np.array(acc_vals)\n",
    "    auc_vals = np.array(auc_vals)\n",
    "    print(acc_vals)\n",
    "    print(auc_vals)\n",
    "\n",
    "    acc_avg = np.round(np.mean(acc_vals), decimals=6)\n",
    "    acc_std = np.round(np.std(acc_vals), decimals=6)\n",
    "    auc_avg = np.round(np.mean(auc_vals), decimals=6)\n",
    "    auc_std = np.round(np.std(auc_vals), decimals=6)\n",
    "\n",
    "    out = \"\\\\avgvar{\" + str(acc_avg) + \"}{\" + str(acc_std) + \"} &\"\n",
    "    out += \" \\\\avgvar{\" + str(auc_avg) + \"}{\" + str(auc_std) + \"} \\n\\n\"\n",
    "    print(out)\n",
    "\n",
    "    print(\"\\n---------------------------------------\\n\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Higher LR performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "### Best-LR\n",
    "suf = \"i_s_scA_scW_tcA_tcW\"\n",
    "# squirrel\n",
    "ps = ['s', 'sm', 'c', 'at']\n",
    "# ednet\n",
    "# ps = ['single', 'time', 'hashed_skill_id', 'sm', 'bundle_id', 'at'] \n",
    "# eedi\n",
    "# ps = ['time', 'i', 'bundle_id']\n",
    "# junyi_15\n",
    "# ps = ['single', 'time', 'i', 'sm', 'part_id']\n",
    "\n",
    "# InformedLR\n",
    "# squirrel \n",
    "# suf = \"i_icA_TW_icW_TW_lag_time_cat_n_gram_postcA_postcW_precA_precW_prev_\"\\\n",
    "#     + \"resp_time_cat_rc_s_scA_TW_scW_TW_sm_t_tcA_TW_tcW_TW_user_avg_correct_vw\"\n",
    "# ps = [\"sm\", \"c\"]\n",
    "# ednet\n",
    "# suf = \"i_icA_TW_icW_TW_lag_time_cat_n_gram_partcA_partcW_\" \\\n",
    "#       \"prev_resp_time_cat_s_scA_TW_scW_TW_sm_tcA_TW_tcW_TW_user_avg_correct_vw\"\n",
    "# ps = ['time', 'sm']\n",
    "# eedi\n",
    "# suf = \"bundle_i_icA_TW_icW_TW_n_gram_precA_precW_s_scA_TW_scW_TW_sm\" + \\\n",
    "#     \"_tcA_TW_tcW_TW_tea_user_avg_correct\"\n",
    "# ps = ['time', 'sm']\n",
    "# junyi\n",
    "# suf = \"hour_i_icA_TW_icW_TW_lag_time_cat_n_gram_postcA_postcW_precA_\" \\\n",
    "#     + \"precW_prev_resp_time_cat_rc_s_scA_TW_scW_TW_sm_tcA_TW_tcW_TW_\" \\\n",
    "#     + \"user_avg_correct\"\n",
    "# ps = ['time', 'sm', 'part_id']\n",
    "\n",
    "\n",
    "print(dataset)\n",
    "print(\"-----------------------------\")\n",
    "acc_vals, auc_vals = [], []\n",
    "for split_id in range(5):\n",
    "    print(\"Split\", split_id)\n",
    "    acc_train, auc_train, acc_test, auc_test = \\\n",
    "        comb_lr_performance(ps, dataset, suf, splits, split_id)\n",
    "    acc_vals.append(acc_test)\n",
    "    auc_vals.append(auc_test)\n",
    "\n",
    "acc_vals = np.array(acc_vals)\n",
    "auc_vals = np.array(auc_vals)\n",
    "print(acc_vals)\n",
    "print(auc_vals)\n",
    "\n",
    "acc_avg = np.round(np.mean(acc_vals), decimals=6)\n",
    "acc_std = np.round(np.std(acc_vals), decimals=6)\n",
    "auc_avg = np.round(np.mean(auc_vals), decimals=6)\n",
    "auc_std = np.round(np.std(auc_vals), decimals=6)\n",
    "\n",
    "print(\"\\n---------------------------------------\\n\")\n",
    "\n",
    "out = \"\\\\avgvar{\" + str(acc_avg) + \"}{\" + str(acc_std) + \"} &\"\n",
    "out += \" \\\\avgvar{\" + str(auc_avg) + \"}{\" + str(auc_std) + \"} \\n\\n\"\n",
    "\n",
    "print(ps)\n",
    "print(\"\\n---------------------------------------\\n\")\n",
    "print(out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "squirrel\n",
      "-----------------------------\n",
      "Split 0\n",
      "s\n",
      "sm\n",
      "c\n",
      "at\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr0/home/rschmuck/miniconda3/envs/vedu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1357: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]Split 1\n",
      "s\n",
      "sm\n",
      "c\n",
      "at\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr0/home/rschmuck/miniconda3/envs/vedu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1357: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]Split 2\n",
      "s\n",
      "sm\n",
      "c\n",
      "at\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr0/home/rschmuck/miniconda3/envs/vedu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1357: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]Split 3\n",
      "s\n",
      "sm\n",
      "c\n",
      "at\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr0/home/rschmuck/miniconda3/envs/vedu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1357: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear]Split 4\n",
      "s\n",
      "sm\n",
      "c\n",
      "at\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr0/home/rschmuck/miniconda3/envs/vedu/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1357: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LibLinear][0.76083299 0.76063921 0.76062471 0.76033632 0.76147323]\n",
      "[0.79170883 0.79126132 0.79139794 0.79113294 0.79119533]\n",
      "\n",
      "---------------------------------------\n",
      "\n",
      "['s', 'sm', 'c', 'at']\n",
      "\n",
      "---------------------------------------\n",
      "\n",
      "\\avgvar{0.760781}{0.000381} & \\avgvar{0.791339}{0.000205} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}